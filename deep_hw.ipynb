{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep-hw.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMNo8UIKC7E8FxXl5HDo5OZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DamnTT/deep-learning/blob/main/deep_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "bZ974d9o7JO9",
        "outputId": "608d5172-9b0a-4e6a-cfa0-d7e85a9a0c82"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# 繪圖相關套件\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.gridspec as gridspec\n",
        "import seaborn as sns\n",
        "plt.style.use( 'ggplot' ) \n",
        "# 標籤編碼(Label)、獨熱編碼(OneHot)\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder \n",
        "# confusion matrix\n",
        "from sklearn import metrics\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "warnings.filterwarnings( 'ignore' )\n",
        "#上傳資料\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "#匯入資料\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "compare = pd.read_csv('gender_submission.csv')\n",
        "submit = compare"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f9ce32b0-7b4c-466f-afeb-e298d9d7de9c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f9ce32b0-7b4c-466f-afeb-e298d9d7de9c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving gender_submission.csv to gender_submission (3).csv\n",
            "Saving train.csv to train (3).csv\n",
            "Saving test.csv to test (3).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JY6X7q9ze9Il"
      },
      "source": [
        "#欄位型態\n",
        "# 定義判別欄位型態的函數\n",
        "def Col_Types( Data ):\n",
        "    Column_Types = Data.dtypes.to_frame().reset_index()   # 判別每個欄位的型態 \n",
        "    Column_Types.columns = ['ColumnName','Type']\n",
        "    Column_Types.sort_values( by='Type', inplace=True ) \n",
        "    return Column_Types\n",
        "\n",
        "# 缺漏植\n",
        "# 定義用來統計欄位缺漏值總數的函數\n",
        "def Missing_Counts( Data ) : \n",
        "    missing = Data.isnull().sum()  # 計算欄位中缺漏值的數量 \n",
        "    missing = missing[ missing>0 ]\n",
        "    missing.sort_values( inplace=True ) \n",
        "    \n",
        "    Missing_Count = pd.DataFrame( { 'ColumnName':missing.index, 'MissingCount':missing.values } )  # Convert Series to DataFrame\n",
        "    Missing_Count[ 'Percentage(%)' ] = Missing_Count['MissingCount'].apply( lambda x:round(x/Data.shape[0]*100,2) )\n",
        "    return  Missing_Count\n",
        "\n",
        "#資料分析\n",
        "# 合併train及test的資料 \n",
        "df_data  = df_train.append( df_test )\n",
        "f_data = compare\n",
        "\n",
        "#生還者比例\n",
        "Survived_Counts = df_data['Survived'].value_counts().reset_index()\n",
        "Survived_Counts.columns = ['Survived','Counts']\n",
        "\n",
        "\n",
        "#相關係數\n",
        "# Survied 與其他欄位間的相關係數\n",
        "Corr_Matrix = df_train.corr()  # 計算相關係數\n",
        "Corr = Corr_Matrix.loc['Survived',:].sort_values()[:-1]\n",
        "Corr = pd.DataFrame({ 'Survived':Corr })\n",
        "\n",
        "#生存率\n",
        "# Sex性別、Pclass票務艙、Embarked登船港口、SibSp兄弟姊妹配偶人數、Parch父母子女人數\n",
        "selected_cols = ['Sex','Pclass','Embarked','SibSp','Parch']\n",
        "\n",
        "for col in selected_cols:\n",
        "    l = ['Survived']\n",
        "    l.append(col) \n",
        "    Survival_Rate = df_data[l].groupby(by=col).mean().round(4).reset_index()\n",
        "    Survival_Rate.columns = [col,'Survival Rate(%)']\n",
        "    Survival_Rate['Survival Rate(%)'] = Survival_Rate['Survival Rate(%)'].map( lambda x:x*100 )\n",
        "    # display( Survival_Rate )   \n",
        "\n",
        "# 創造新的特徵變數：家庭人數(Family_Size)\n",
        "df_data['Family_Size'] = df_data['SibSp'] + df_data['Parch'] + 1\n",
        "\n",
        "Survival_Rate = df_data[['Family_Size','Survived']].groupby(by=['Family_Size']).agg(np.mean)*100\n",
        "Survival_Rate.columns = ['Survival Rate(%)']\n",
        "Survival_Rate.reset_index()\n",
        "\n",
        "df_data[ 'Family_Class' ] = np.nan\n",
        "\n",
        "df_data.loc[ df_data.Family_Size==0, 'Family_Class' ] = 2\n",
        "df_data.loc[ (df_data.Family_Size>=1) & (df_data.Family_Size<=3), 'Family_Class' ] = 3\n",
        "df_data.loc[ (df_data.Family_Size>=4) & (df_data.Family_Size<=6), 'Family_Class' ] = 2\n",
        "df_data.loc[ (df_data.Family_Size>=7), 'Family_Class' ] = 1\n",
        "\n",
        "# Sex & Pclass \n",
        "Survival_Rate = df_data[['Sex','Pclass','Survived']].groupby(by=['Sex','Pclass']).agg(np.mean)*100\n",
        "Survival_Rate.columns = ['Survival Rate(%)']\n",
        "Survival_Rate.reset_index()\n",
        "df_data[ 'Sex_Pclass' ] = np.nan\n",
        "df_data.loc[ (df_data.Sex=='female') & (df_data.Pclass==1), 'Sex_Pclass' ] = 2\n",
        "df_data.loc[ (df_data.Sex=='female') & (df_data.Pclass==2), 'Sex_Pclass' ] = 3\n",
        "df_data.loc[ (df_data.Sex=='female') & (df_data.Pclass==3), 'Sex_Pclass' ] = 3\n",
        "df_data.loc[ (df_data.Sex=='male') & (df_data.Pclass==1), 'Sex_Pclass' ] = 1\n",
        "df_data.loc[ (df_data.Sex=='male') & (df_data.Pclass==2), 'Sex_Pclass' ] = 1\n",
        "df_data.loc[ (df_data.Sex=='male') & (df_data.Pclass==3), 'Sex_Pclass' ] = 2\n",
        "\n",
        "\n",
        "#姓名\n",
        "# Method 1: split()\n",
        "df_data['Title'] = df_data.Name.str.split(', ', expand=True)[1]\n",
        "df_data['Title'] = df_data.Title.str.split('.', expand=True)[0]\n",
        "df_data['Title'].unique()\n",
        "\n",
        "# Method 2: 正規表示法(Regular Expression)\n",
        "import re\n",
        "\n",
        "regex = re.compile( ' ([A-Za-z]+)\\.' )  \n",
        "df_data['Title'] = df_data.Name.map( lambda x:regex.search(x)[0] )\n",
        "# Dropping the first and the last words\n",
        "df_data['Title'] = df_data.Title.map( lambda x:x[1:][:-1] )  \n",
        "df_data['Title'].unique()\n",
        "df_data['Title'] = df_data.Title.replace( ['Don','Rev','Dr','Major','Lady','Sir','Col','Capt','Countess','Jonkheer','Dona'], 'Rare' )\n",
        "df_data['Title'] = df_data.Title.replace( ['Ms','Mlle'], 'Miss' )\n",
        "df_data['Title'] = df_data.Title.replace( 'Mme', 'Mrs' )\n",
        "df_data['Title'].unique()\n",
        "# 刪除原始資料中的 Name 欄位 \n",
        "df_data.drop( 'Name', axis=1, inplace=True )\n",
        "#船票號碼\n",
        "df_data['Ticket_info'] = df_data.Ticket.apply( lambda x:x.replace('.','').replace('/','').strip().split(' ')[0] if not x.isdigit() else 'X')\n",
        "df_data['Ticket_info'].unique()\n",
        "# 刪除原始資料中的 Ticket 欄位 \n",
        "df_data.drop( 'Ticket', axis=1, inplace=True )\n",
        "#填補缺漏值(Missing Values)\n",
        "Missing_Counts( df_data.drop('Survived', axis=1) )\n",
        "#票價\n",
        "df_data['Fare'].fillna( df_data.Fare.median(), inplace=True )\n",
        "#登船港口(Embarked) : 填補次數最多的港口 'S'\n",
        "# 計算 Embarked 欄位中每個相異值的次數\n",
        "# display( df_data['Embarked'].value_counts() )\n",
        "# 填補 Embarked 欄位的缺漏值\n",
        "df_data['Embarked'].fillna( 'S', inplace=True )\n",
        "#年齡\n",
        "# 新增標註 Age 欄位是否為缺漏值的欄位(有缺漏標為0)\n",
        "df_data['isAge'] = df_data['Age'].isnull().map( lambda x:0 if x==True else 1 )\n",
        "# 分別觀察 Age 與 Sex、Pclass 的缺漏值分布狀況\n",
        "\n",
        "# 計算每個 Title 的年齡平均值及中位數\n",
        "Age_Mean = df_data[['Title','Age']].groupby( by=['Title'] ).mean()\n",
        "Age_Median = df_data[['Title','Age']].groupby( by=['Title'] ).median()\n",
        "Age_Mean.columns = ['Age Mean']\n",
        "Age_Median.columns = ['Age Median']\n",
        "Age_Mean.reset_index( inplace=True )\n",
        "Age_Median.reset_index( inplace=True )\n",
        "\n",
        "# 利用每個 Title 的年齡平均數，填補每個 Title 所對應 Age 的缺漏值\n",
        "df_data.loc[(df_data.Age.isnull())&(df_data.Title=='Master'),'Age'] = Age_Mean.loc[Age_Mean.Title=='Master','Age Mean'][0]\n",
        "df_data.loc[(df_data.Age.isnull())&(df_data.Title=='Miss'),'Age'] = Age_Mean.loc[Age_Mean.Title=='Miss','Age Mean'][1]\n",
        "df_data.loc[(df_data.Age.isnull())&(df_data.Title=='Mr'),'Age'] = Age_Mean.loc[Age_Mean.Title=='Mr','Age Mean'][2]\n",
        "df_data.loc[(df_data.Age.isnull())&(df_data.Title=='Mrs'),'Age'] = Age_Mean.loc[Age_Mean.Title=='Mrs','Age Mean'][3]\n",
        "df_data.loc[(df_data.Age.isnull())&(df_data.Title=='Rare'),'Age'] = Age_Mean.loc[Age_Mean.Title=='Rare','Age Mean'][4]\n",
        "\n",
        "# 刪除 isAge 欄位 \n",
        "df_data.drop( 'isAge', axis=1, inplace=True )\n",
        "\n",
        "# 乘客年齡是否未滿17歲(是：1；否：0)\n",
        "df_data[ 'is_Age_17' ] = (df_data.Age<17)*1\n",
        "\n",
        "# 觀察 Cabin 中的種類\n",
        "df_data['Cabin'].unique()\n",
        "\n",
        "# 取出 Cabin 中的第一個字母，如果為缺漏值，則以 NoCabin 表示\n",
        "df_data['Cabin'] = df_data['Cabin'].apply( lambda x:str(x)[0] if not pd.isnull(x) else 'NoCabin' )\n",
        "df_data['Cabin'].unique()\n",
        "Missing_Counts( df_data.drop('Survived', axis=1) )\n",
        "\n",
        "# 對 Fare 欄位取對數\n",
        "df_data['LogFare'] = np.log1p( df_data.Fare )\n",
        "\n",
        "# 計算 Fare 欄位各個百分位數(Percentile)\n",
        "P_all = [ np.percentile( df_data.Fare, q=i ) for i in np.arange(0,101) ] \n",
        "Pth_Percentile = pd.DataFrame( { 'Q':list(range(101)), 'Value':P_all } )\n",
        "\n",
        "# The first、second and third quartile(i,e., the 25th、50th and 75th Percentile)\n",
        "Q1 = Pth_Percentile.iloc[ 25, 1 ]\n",
        "Q2 = Pth_Percentile.iloc[ 50, 1 ]\n",
        "Q3 = Pth_Percentile.iloc[ 75, 1 ]\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# 依照四分位數，對 Fare 欄位進行分組\n",
        "Fare_bin = [ 0, Q1, Q2, Q3, Q3+1.5*IQR, df_data.Fare.max() ]\n",
        "df_data[ 'Fare_Group' ] = pd.cut( df_data.Fare.values, Fare_bin )\n",
        "\n",
        "# 計算每個分組中的資料筆數\n",
        "Group_Counts = df_data[ 'Fare_Group' ].value_counts().reset_index()    \n",
        "Group_Counts.columns = [ 'Fare_Group', 'Counts' ]\n",
        "Group_Counts.sort_values( by='Fare_Group' )\n",
        "\n",
        "# 刪除 Fare_Group 欄位 \n",
        "df_data.drop( ['Fare','Fare_Group'], axis=1, inplace=True )\n",
        "\n",
        "# OneHot Encoding\n",
        "OneHot_Embarked = pd.get_dummies( df_data.Embarked, prefix='Embarked' )\n",
        "\n",
        "# 合併 Embarked 編碼後的欄位\n",
        "df_data = pd.concat( [ df_data, OneHot_Embarked ], axis=1 )\n",
        "df_data.drop( 'Embarked', axis=1, inplace=True )\n",
        "\n",
        "# Label Encoding\n",
        "Sex_mapping = { 'male':0, 'female':1 }\n",
        "df_data[ 'Sex' ] = df_data.Sex.map( Sex_mapping )\n",
        "\n",
        "# 檢視特徵工程後的資料\n",
        "# print( f'Shape of data after feature engineering = {df_data.shape}' )\n",
        "for col in ['Title','Ticket_info','Cabin']:\n",
        "    df_data[col] = df_data[col].astype('category').cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-GyhW_RCnYH"
      },
      "source": [
        "# 產生訓練集和測試集\n",
        "Train = df_data[ pd.notnull(df_data.Survived) ]\n",
        "Test = df_data[ pd.isnull(df_data.Survived) ]\n",
        "\n",
        "#分類器字典\n",
        "classifier_dict = dict() \n",
        "\n",
        "# 訓練集刪除 PassengerId 欄位；\n",
        "# 測試集刪除 PassengerId 與 Survived 欄位\n",
        "Train.drop( ['PassengerId'], axis=1, inplace=True )\n",
        "Test.drop( ['PassengerId','Survived'], axis=1, inplace=True )\n",
        "\n",
        "# 將測試集中的標籤欄位 Survived 單獨拆出\n",
        "Y_Train = Train.Survived\n",
        "X_Train = Train.drop( ['Survived'], axis=1 )\n",
        "ftest = f_data.Survived\n",
        "\n",
        "#資料轉換\n",
        "tmp = np.zeros(418)\n",
        "a = 0\n",
        "for i in ftest:\n",
        "  tmp[a] = i\n",
        "  a = a+1\n",
        "ftest = tmp\n",
        "\n",
        "#求出其accuracy, precision, recall, F1 score, confusion matrix等資訊\n",
        "def deep_classified(Test_pred, Classifier):\n",
        "  confusion = metrics.confusion_matrix(ftest, Test_pred)\n",
        "  TP = confusion[1, 1]\n",
        "  TN = confusion[0, 0]\n",
        "  FP = confusion[0, 1]\n",
        "  FN = confusion[1, 0]\n",
        "  print(\"This is the \",Classifier,\"classifier`:\")\n",
        "  # Classification Error: Overall, how does the classifier predict incorrectly (Misclassification Rate)\n",
        "  print(\"Accuracy: \",(FP + FN) / float(TP + TN + FP + FN))\n",
        "  # Specificity: When the actual value is negative, how often is the prediction correct?\n",
        "  # print(\"Specificity\",TN / float(TN + FP))\n",
        "  # Precision: When a positive value is predicted, how often is the prediction correct?\n",
        "  precision = TP / float(TP + FP)\n",
        "  print(\"Precision：\", TP / float(TP + FP))\n",
        "  # Recall: When the actual value is positive, how often is the prediction correct?\n",
        "  recall = TP / float(TP + FN)\n",
        "  print(\"Recall：\",TP / float(TP + FN))\n",
        "  # F1 Score\n",
        "  print(\"F1 Score：\", (2 * (precision * recall) / (precision + recall)))\n",
        "  # Confusion matrix\n",
        "  print(\"Confusion matrix：\",\"\\n\",confusion)\n",
        "  print(\"\\n\")\n",
        "\n",
        "# 提交檔案\n",
        "def submit_file(Test_pred, Classifier):\n",
        "  submit['Survived'] = Test_pred.astype(int)\n",
        "  submit.to_csv( 'Titanic_RandomForest.csv', index=False )\n",
        "  # print(Classifier,\"classifier\" + f'預測結果：')\n",
        "  print( f'預測結果：' )\n",
        "  submit "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVXtn50hfFHa",
        "outputId": "1067e936-5464-4375-8ff1-98464bbb7e9c"
      },
      "source": [
        "# Logistic Classifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LC = LogisticRegression()\n",
        "LC.fit(X_Train[SelectedFeatures],Y_Train)\n",
        "Test_pred_LC = LC.predict( Test[SelectedFeatures] )\n",
        "classifier_dict[\"Logistic\"] = Test_pred_LC #放入字典\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "NBC = GaussianNB()\n",
        "NBC.fit(X_Train[SelectedFeatures], Y_Train)\n",
        "Test_pred_NBC = NBC.predict( Test[SelectedFeatures] )\n",
        "classifier_dict[\"Naïve Bayes\"] = Test_pred_NBC #放入字典\n",
        "\n",
        "# 決策樹(Decision Tree)\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "DTC = RandomForestClassifier( )\n",
        "DTC.fit( X_Train[SelectedFeatures], Y_Train )  # 自變數、應變數進行擬合\n",
        "Test_pred_DTC = DTC.predict( Test[SelectedFeatures] )\n",
        "classifier_dict[\"Decision Tree\"] = Test_pred_DTC #放入字典\n",
        "\n",
        "# 隨機森林參數：\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# n_estimators: 樹的數量(default=10)。\n",
        "# min_samples_leaf: 最終葉節點最少樣本數(default=1)；\n",
        "#                   當樣本不大時，可不設定使用預設，若樣本數量非常大時，則推薦增加此參數值。\n",
        "# min_samples_split:節點再劃分時所需的最小樣本數(default=2)；\n",
        "#                   當樣本不大時，可不設定使用預設，若樣本數量非常大時，則推薦增加此參數值。\n",
        "# oob_score: 是否採用袋外樣本(out-of-bag samples)來評估模型的準確度(default=False)。\n",
        "RFC = RandomForestClassifier( n_estimators = 1000,\n",
        "                              min_samples_split = 20,\n",
        "                              min_samples_leaf = 1,\n",
        "                              oob_score = True,\n",
        "                              random_state = 1,\n",
        "                              n_jobs = -1 ) \n",
        "RFC_2 = RandomForestClassifier( n_estimators = 1000,\n",
        "                                min_samples_split = 20,\n",
        "                                min_samples_leaf = 1,\n",
        "                                oob_score = True,\n",
        "                                random_state = 1,\n",
        "                                n_jobs = -1 ) \n",
        "\n",
        "# 篩選部份特徵欄位餵入模型進行訓練\n",
        "SelectedFeatures = ['Age','Sex','LogFare','Title','Pclass','Sex_Pclass']\n",
        "RFC_2.fit( X_Train[SelectedFeatures], Y_Train )\n",
        "\n",
        "# 預測測試集資料\n",
        "Test_pred_RFC = RFC_2.predict( Test[SelectedFeatures] )\n",
        "classifier_dict[\"Random Forest\"] = Test_pred_RFC #放入字典\n",
        "\n",
        "# 建立 XGBClassifier 模型\n",
        "from xgboost import XGBClassifier\n",
        "XGBC = XGBClassifier(n_estimators=100, learning_rate= 0.3)\n",
        "XGBC.fit(X_Train[SelectedFeatures], Y_Train)  # 使用訓練資料訓練模型\n",
        "Test_pred_XGBC = XGBC.predict(Test[SelectedFeatures]) # 使用訓練資料預測分類\n",
        "classifier_dict[\"XGBoost\"] = Test_pred_XGBC\n",
        "\n",
        "# 建立 MLPClassifier 模型\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "MLPC = MLPClassifier(random_state=1, max_iter=400)\n",
        "MLPC.fit(X_Train[SelectedFeatures],Y_Train)\n",
        "Test_pred_MLPC = MLPC.predict(Test[SelectedFeatures])\n",
        "classifier_dict[\"MLP\"] = Test_pred_MLPC\n",
        "\n",
        "#把結果印出來\n",
        "for i in classifier_dict:\n",
        "  deep_classified(classifier_dict[i], i)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the  Logistic classifier`:\n",
            "Accuracy:  0.09569377990430622\n",
            "Precision： 0.8589743589743589\n",
            "Recall： 0.881578947368421\n",
            "F1 Score： 0.8701298701298701\n",
            "Confusion matrix： \n",
            " [[244  22]\n",
            " [ 18 134]]\n",
            "\n",
            "\n",
            "This is the  Naïve Bayes classifier`:\n",
            "Accuracy:  0.07894736842105263\n",
            "Precision： 0.8216216216216217\n",
            "Recall： 1.0\n",
            "F1 Score： 0.9020771513353116\n",
            "Confusion matrix： \n",
            " [[233  33]\n",
            " [  0 152]]\n",
            "\n",
            "\n",
            "This is the  Decision Tree classifier`:\n",
            "Accuracy:  0.16985645933014354\n",
            "Precision： 0.7515527950310559\n",
            "Recall： 0.7960526315789473\n",
            "F1 Score： 0.7731629392971247\n",
            "Confusion matrix： \n",
            " [[226  40]\n",
            " [ 31 121]]\n",
            "\n",
            "\n",
            "This is the  Random Forest classifier`:\n",
            "Accuracy:  0.11004784688995216\n",
            "Precision： 0.8680555555555556\n",
            "Recall： 0.8223684210526315\n",
            "F1 Score： 0.8445945945945945\n",
            "Confusion matrix： \n",
            " [[247  19]\n",
            " [ 27 125]]\n",
            "\n",
            "\n",
            "This is the  XGBoost classifier`:\n",
            "Accuracy:  0.1339712918660287\n",
            "Precision： 0.8157894736842105\n",
            "Recall： 0.8157894736842105\n",
            "F1 Score： 0.8157894736842104\n",
            "Confusion matrix： \n",
            " [[238  28]\n",
            " [ 28 124]]\n",
            "\n",
            "\n",
            "This is the  MLP classifier`:\n",
            "Accuracy:  0.0430622009569378\n",
            "Precision： 0.9527027027027027\n",
            "Recall： 0.9276315789473685\n",
            "F1 Score： 0.9400000000000001\n",
            "Confusion matrix： \n",
            " [[259   7]\n",
            " [ 11 141]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}